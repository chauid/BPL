## Usage
|사용 메서드|반환 자료형|설명|선행 조건|
|---|:---:|---|---|
|clearModel|void|모델 정보 초기화|없음|
|loadModel|void|기존 모델 불러오기|없음|
|addDenseLayer|void|은닉층 추가|없음|
|addDropoutLayer|void||없음|
|readInputData|void|학습 데이터 읽기|없음|
|printInputData|void|입력된 학습 데이터 출력|readInputData|
|saveModel|void|모델 데이터 저장|readInputData|
|prepare|void|모델 학습 준비|readInputData, (loadModel or addLayer)|
|printModel|void|모델 구조 출력|(loadModel or prepare)|
|learning|void|모델 학습|(loadModel or prepare)|
|calcLoss|double|전체 데이터셋의 오차율 평균 계산|(loadModel or prepare)|
|predict|vector<double>|값 예측|(loadModel or prepare)|
|predictFromFile|vector<vector<double>>|테스트 데이터 파일에서 값 예측|(loadModel or prepare)|
|predictToFile|void|예측값 데이터 파일로 출력|(loadModel or prepare)|
|predictToFileFromFile|void|테스트 데이터 파일에서 예측값 데이터 파일로 출력|(loadModel or prepare)|

> [!WARNING]
> prepare 후 loadModel를 실행하면 현재 모델 구조와 가중치 정보를 불러오는 기존의 모델로 덮어씁니다. (모델 구조 및 가중치값 손실)  
> 기존에 학습한 모델의 정보를 저장(saveModel) 후 loadModel을 사용하십시오.  
> **손실된 값은 복구할 수 없습니다.**  

## Configuration
`input.data`
> [입력 길이] [출력 길이]  
> 입력값1 입력값2 ... 출력값1 출력값2

### e.g.  
입력값: [[0, 0], [0, 1], [1, 0], [1, 1]]  
출력값: [0, 1, 1, 0]  
_(리스트의 개수는 학습데이터의 개수를 의미)_  
```
** input.data **
2 1
0 0 0
0 1 1
1 0 1
1 1 0
```

<hr />

`test.data`
> [입력 길이]
> 입력값1 입력값2 ...

`input.data`에서 출력값만 제외한 형태
```
** test.data **
2
0 0
0 1
1 0
1 1
```

<hr />

`model.data`
> [입력 길이] [은닉층 개수]  
> [각 은닉층의 노드 수: 1레이어 노드 수, 2레이어 노드수, ...]  
> [각 은닉층의 활성화 함수: 1레이어 활성화 함수, 2레이어 활성화 함수, ...]  
> [weight_init] [optimizer] [loss_function] [learning_rate]  
> [w11, w12, ..., w1n] [bias_1] < `1번째 레이어의 Weight 행렬`  
> [w21, w22, ..., w2n] [bias_2]  
> [w... ] [bias... ]  
> [wm1, w22, ..., wmn] [bias_m]  
> 
> [M11, M12, ..., M1n] < `1번째 레이어의 Momentum 행렬`  
> [M21, M22, ..., M2n]  
> [M... ]  
> [Mm1, M22, ..., Mmn]  
> 
> [G11, G12, ..., G1n] < `1번째 레이어의 SquaredGrandient 행렬`  
> [G21, G22, ..., G2n]  
> [G... ]  
> [Gm1, G22, ..., Gmn]  
> 
> [w11, w12, ..., w1n] [bias_1] < `2번째 레이어의 Weight 행렬`  
> [w21, w22, ..., w2n] [bias_2]  
> [ w... ] [ bias... ]  
> [wm1, w22, ..., wmn] [bias_m]  
> 
> . . .  

_','는 편의상 README에서만 표현.(파일에는 ','를 넣지 않음)_  
k번째 은닉층에서 출발노드(k-1번 은닉층) 인덱스를 n, 도착노드(k번째 은닉층) 인덱스를 m으로 표시  
k번째 은닉층의 편향값 개수 = k번째 은닉층의 노드 수  
모델을 로드 후 이어서 학습하기 위해 속도 행렬(학습의 진행도)를 저장함.  

|Active Function(활성화 함수 목록)|Value|
|---|---|
|Sigmoid|0|
|HyperbolicTangent|1|
|Softmax|2|
|LeRU|3|
|leakyReLU|4|
|ELU|5|

|Weight Initialization(가중치 초기화 방법)|Value|
|---|---|
|ZeroInitialize|0|
|NormalDistribution|1|
|UniformDistribution|2|
|XavierUniformDistribution|3|
|XavierNormalDistribution|4|
|He|5|

|Optimizer(최적화 기법 목록)|Value|
|---|---|
|GD|0|
|SGD|1|
|Momentum|2|
|AdaGrad|3|
|RMSProps|4|
|Adam|5|

|Loss Function(손실 함수 목록)|Value|
|---|---|
|MSE|0|
|BinaryCrossEntropyLoss|1|
|CategoricalCrossEntropyLoss|2|
|SparseCrossEntropyLoss|3|

### e.g.  
|Layer|(Input, Output) Shape|Param|
|---|---|---|
|Input|(0, 2)|None| 
|Hidden 0|(2, 6)|18|
|Hidden 1|(6, 12)|84|
|(Output)Hidden 2|(12, 1)|13|


위와 같은 모델 구조에 1번 레이어의 활성화 함수를 ReLU, 2번 레이어의 활성화 함수를 ReLU, 출력 레이어의 활성화 함수를 Sigmoid로 설정  
weight_init: XavierNormalDistribution,  
optimizer: Adam,  
loss_function: MSE,  
learning_rate: 0.001,   
```
** model.dat **
2 3
6 12 1
3 3 0
4 5 0 0.0010
-0.022383 -0.023287 0.000000
0.665843 0.997836 0.000000
0.676810 -0.677009 0.000000
-0.655633 1.005036 0.000000
0.728249 -0.728794 0.000000
2.066987 0.042426 0.000000

0.000000 0.000000
-0.000016 -0.000035
-0.000010 0.000012
0.000036 -0.000071
-0.000017 0.000012
-0.000075 0.000004

0.000000 0.000000
0.000000 0.000001
0.000003 0.000002
0.000002 0.000013
0.000002 0.000001
0.000003 0.000014

0.032805 0.081036 0.003759 1.175746 0.376211 -0.258737 0.000000
0.545126 0.136910 0.091602 -0.114870 0.288281 -0.502102 0.000000
-0.057609 0.326771 1.361283 0.606125 1.690864 -0.358213 0.000000
0.867635 0.557718 -0.459080 -0.480822 -1.083657 0.764675 0.000000
-0.299946 0.069909 0.871666 0.506013 0.701526 -0.138968 0.000000
0.476202 -0.120370 0.725953 0.456136 0.939887 0.019337 0.000000
0.272132 0.700474 -0.335327 1.054185 -0.082104 -0.727121 0.000000
0.476470 -0.028140 -0.436053 0.523351 -0.687548 0.715996 0.000000
-0.646862 0.052424 0.571470 0.136181 0.397474 -0.063893 0.000000
-0.428441 -0.064980 1.095251 -0.372141 1.319441 0.112699 0.000000
-0.197372 0.041278 -0.046064 -0.158653 0.336086 -0.240060 0.000000
0.130732 -0.131039 0.791358 -0.761181 0.965867 0.229295 0.000000

0.000000 -0.000025 0.000000 -0.000027 0.000000 0.000002
0.000000 0.000001 0.000000 0.000001 0.000000 0.000000
0.000000 -0.000023 -0.000004 -0.000035 -0.000004 0.000011
0.000000 -0.000057 0.000003 0.000016 0.000004 -0.000110
0.000000 -0.000011 -0.000002 -0.000017 -0.000002 0.000005
0.000000 -0.000006 -0.000001 -0.000006 -0.000001 -0.000001
0.000000 -0.000009 0.000000 -0.000026 0.000000 0.000025
0.000000 -0.000032 0.000002 0.000009 0.000002 -0.000061
0.000000 0.000006 -0.000001 -0.000007 -0.000001 0.000019
0.000000 -0.000002 -0.000003 0.000000 -0.000003 -0.000007
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
0.000000 -0.000002 -0.000003 0.000000 -0.000003 -0.000008

0.000000 0.000003 0.000000 0.000002 0.000000 0.000006
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
0.000000 0.000004 0.000000 0.000003 0.000000 0.000007
0.000000 0.000008 0.000000 0.000005 0.000000 0.000020
0.000000 0.000002 0.000000 0.000001 0.000000 0.000005
0.000000 0.000001 0.000000 0.000000 0.000000 0.000001
0.000000 0.000001 0.000000 0.000001 0.000000 0.000001
0.000000 0.000004 0.000000 0.000003 0.000000 0.000011
0.000000 0.000002 0.000000 0.000002 0.000000 0.000007
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
0.000000 0.000000 0.000000 0.000000 0.000000 0.000000
0.000000 0.000000 0.000000 0.000000 0.000000 0.000001

1.130660 -0.218964 1.568480 -1.453034 0.762081 0.243754 1.214791 -0.812585 0.456874 1.072477 0.961170 1.170499 0.000000

-0.000031 -0.000000 -0.000028 0.000089 -0.000017 -0.000012 -0.000042 0.000050 -0.000007 -0.000007 0.000000 -0.000006

0.000002 0.000000 0.000001 0.000006 0.000001 0.000005 0.000002 0.000009 0.000002 0.000000 0.000000 0.000000


```